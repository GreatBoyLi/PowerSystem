import xarray as xr
import pandas as pd
import numpy as np
import os
import glob
from GPTPV.utils.config import load_config

config_file = "../config/config.yaml"
config = load_config(config_file)

# --- é…ç½®ä¿®æ”¹ ---
# 1. è¿™é‡Œç°åœ¨æŒ‡å‘åŒ…å« nc æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„
ERA5_DIR = config["file_paths"]["era5_dir"]
OUTPUT_CSV = config["file_paths"]["era5_output"]
REAL_STATIONS = config["stations"]["real_stations"]
POINTS_PER_STATION = config["stations"]["virtual_points_per_station"]

# 2. è·å–æ—¶é—´èŒƒå›´
START_DATE = pd.to_datetime(config["dates"]["start_date"])
END_DATE = pd.to_datetime(config["dates"]["end_date"])


# ===========================================

def get_relevant_era5_files(data_dir, start_date, end_date):
    """
    æ ¹æ®å¼€å§‹å’Œç»“æŸæ—¥æœŸï¼Œç­›é€‰å‡ºéœ€è¦è¯»å–çš„ .nc æ–‡ä»¶åˆ—è¡¨ã€‚
    æ–‡ä»¶åæ ¼å¼å‡è®¾ä¸º: era5_shanxi_YYYY_MM.nc
    """
    all_files = sorted(glob.glob(os.path.join(data_dir, "era5_shanxi_*.nc")))
    selected_files = []

    # ç”Ÿæˆæˆ‘ä»¬éœ€è¦è¦†ç›–çš„å¹´æœˆåˆ—è¡¨ (ä¾‹å¦‚: 2020-01, 2020-02)
    # ä½¿ç”¨ 'MS' (Month Start) é¢‘ç‡ç”Ÿæˆ
    needed_periods = pd.date_range(start=start_date, end=end_date, freq='MS').strftime("%Y_%m")

    # å¦‚æœæ—¶é—´èŒƒå›´åœ¨ä¸€ä¸ªæœˆå†… (ä¾‹å¦‚ 1æœˆ5æ—¥åˆ°1æœˆ10æ—¥)ï¼Œdate_rangeå¯èƒ½ä¸ºç©ºï¼Œæ‰‹åŠ¨è¡¥ä¸Š
    if len(needed_periods) == 0:
        needed_periods = [start_date.strftime("%Y_%m")]
        # å¦‚æœè·¨æœˆä½†æ²¡æ»¡ä¸€ä¸ªæœˆ(ä¾‹å¦‚1æœˆ31åˆ°2æœˆ1æ—¥)ï¼Œéœ€è¦æŠŠç»“æŸæœˆä¹ŸåŠ ä¸Š
        if start_date.strftime("%Y_%m") != end_date.strftime("%Y_%m"):
            needed_periods.append(end_date.strftime("%Y_%m"))

    print(f"ğŸ“… éœ€è¦å¯»æ‰¾çš„æœˆä»½: {list(needed_periods)}")

    for f_path in all_files:
        f_name = os.path.basename(f_path)
        # ç®€å•ç²—æš´åŒ¹é…ï¼šåªè¦æ–‡ä»¶ååŒ…å« "2020_01" è¿™ç§å­—ç¬¦ä¸²å°±é€‰ä¸­
        for period in needed_periods:
            if period in f_name:
                selected_files.append(f_path)
                break

    return sorted(selected_files)


def extract_and_broadcast_era5():
    # 1. ç­›é€‰æ–‡ä»¶
    relevant_files = get_relevant_era5_files(ERA5_DIR, START_DATE, END_DATE)

    if not relevant_files:
        print(f"âŒ åœ¨ {ERA5_DIR} ä¸‹æœªæ‰¾åˆ°åŒ¹é… {START_DATE} åˆ° {END_DATE} çš„æ–‡ä»¶ï¼")
        return

    print(f"ğŸ“‚ å°†åŠ è½½ä»¥ä¸‹ {len(relevant_files)} ä¸ªæ–‡ä»¶:")
    for f in relevant_files:
        print(f"   - {os.path.basename(f)}")

    # 2. ä½¿ç”¨ open_mfdataset åŒæ—¶æ‰“å¼€å¤šä¸ªæ–‡ä»¶å¹¶è‡ªåŠ¨åˆå¹¶æ—¶é—´ç»´åº¦
    print("ğŸ”„ æ­£åœ¨åŠ è½½å¹¶åˆå¹¶æ•°æ®é›†...")
    # chunkså‚æ•°æœ‰åŠ©äºå¤„ç†å¤§æ–‡ä»¶ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º
    try:
        ds = xr.open_mfdataset(relevant_files, combine='by_coords', chunks={'time': 500})
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return

    # 3. æ ‡å‡†åŒ–æ—¶é—´åˆ—å (é˜²æ­¢æœ‰çš„æ–‡ä»¶å« valid_time æœ‰çš„å« time)
    if 'valid_time' in ds.coords:
        ds = ds.rename({'valid_time': 'time'})

    # 4. ğŸ¯ æ ¸å¿ƒæ­¥éª¤ï¼šæ—¶é—´åˆ‡ç‰‡ (Time Slicing)
    # è¿™ä¸€æ­¥åªä¿ç•™ config ä¸­é…ç½®çš„æ—¶é—´æ®µ
    print(f"âœ‚ï¸ æ­£åœ¨è£åˆ‡æ—¶é—´èŒƒå›´: {START_DATE} -> {END_DATE}")
    try:
        ds_sliced = ds.sel(time=slice(START_DATE, END_DATE))
    except Exception as e:
        print(f"âŒ æ—¶é—´è£åˆ‡å¤±è´¥ï¼Œè¯·æ£€æŸ¥ncæ–‡ä»¶å†…çš„æ—¶é—´æ ¼å¼ã€‚é”™è¯¯: {e}")
        return

    if ds_sliced.time.size == 0:
        print("âš ï¸ è£åˆ‡åæ•°æ®ä¸ºç©ºï¼è¯·æ£€æŸ¥ Start/End Date æ˜¯å¦åœ¨æ–‡ä»¶çš„æ—¶é—´èŒƒå›´å†…ã€‚")
        return

    # 5. æ—¶é—´é‡é‡‡æ ·ä¸æ’å€¼ (1h -> 15min) [ä¿ç•™ä¹‹å‰çš„æ ¸å¿ƒé€»è¾‘]
    print("â³ æ­£åœ¨æ‰§è¡Œ 15åˆ†é’Ÿ é¢‘ç‡çš„æ’å€¼å¤„ç†...")
    ds_15min = ds_sliced.resample(time='15min').interpolate('linear')

    # 6. ç‰©ç†é‡è®¡ç®—
    print("ğŸ§® æ­£åœ¨è¿›è¡Œç‰©ç†é‡è®¡ç®—...")
    temp_c = ds_15min['t2m'] - 273.15
    precip_mm = ds_15min['tp'] * 1000
    precip_mm = precip_mm.where(precip_mm >= 0, 0)
    wind_speed = np.sqrt(ds_15min['u10'] ** 2 + ds_15min['v10'] ** 2)

    # 7. æå–ä¸å¹¿æ’­
    # ä¸ºäº†é¿å…å†…å­˜æº¢å‡ºï¼Œå¦‚æœæ•°æ®é‡ç‰¹åˆ«å¤§ï¼Œè¿™é‡Œå¯ä»¥è€ƒè™‘å…ˆ load() è¿›å†…å­˜
    # æˆ–è€…ç›´æ¥è¿›è¡Œè®¡ç®— (xarray æ˜¯æ‡’åŠ è½½çš„)
    print("ğŸš€ æ­£åœ¨æå–å¹¶åˆ†å‘æ•°æ®...")

    # ç¡®ä¿æ—¶é—´ç´¢å¼•æ˜¯ pandas datetime
    time_index = pd.to_datetime(ds_15min.time.values)
    data_dict = {"Timestamp": time_index}

    for station in REAL_STATIONS:
        s_name = station['name']
        s_lat = station['lat']
        s_lon = station['lon']

        # æå– (ä¼šè§¦å‘è®¡ç®—)
        t_val = temp_c.sel(latitude=s_lat, longitude=s_lon, method='nearest').values
        p_val = precip_mm.sel(latitude=s_lat, longitude=s_lon, method='nearest').values
        w_val = wind_speed.sel(latitude=s_lat, longitude=s_lon, method='nearest').values

        for i in range(POINTS_PER_STATION):
            base_col = f"{s_name}_P{i}"
            data_dict[f"{base_col}_Temp"] = t_val
            data_dict[f"{base_col}_Wind"] = w_val
            data_dict[f"{base_col}_Precip"] = p_val

    # 8. ä¿å­˜
    final_df = pd.DataFrame(data_dict)
    final_df = final_df.set_index("Timestamp")

    print(f"===== ğŸ“Š æ•°æ®é¢„è§ˆ (æ—¶é—´èŒƒå›´: {final_df.index.min()} åˆ° {final_df.index.max()}) =====")
    print(final_df.iloc[:, :3].head())

    final_df.to_csv(OUTPUT_CSV)
    print(f"\nâœ… å¤„ç†å®Œæˆï¼å·²ä¿å­˜è‡³: {OUTPUT_CSV}")


if __name__ == "__main__":
    extract_and_broadcast_era5()